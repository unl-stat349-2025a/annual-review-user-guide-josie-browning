[
  {
    "objectID": "task-analysis.html#additional-guidance",
    "href": "task-analysis.html#additional-guidance",
    "title": "Appendix C — Task Analysis",
    "section": "C.1 Additional Guidance",
    "text": "C.1 Additional Guidance\nYour check-in should answer these basic questions (and similar concerns that apply more directly to your topic).\nOnce you’ve completed the check-in, you can use this section to jump-start an introduction/set-up/getting started section in your user guide. This document should remain as an appendix to your main report.\n\n\n\n\nAndrews, Dorothy, Shawna Ackerman, Dennis Kapylou, Alex Esche, Liaw Huang, and Reese Mularz. 2023. “An Actuarial View of Data Bias: Definitions, Impacts, and Considerations,” July. https://www.actuary.org/sites/default/files/2023-07/risk_brief_data_bias.pdf."
  },
  {
    "objectID": "guide.html",
    "href": "guide.html",
    "title": "1  User Guide To Bias in Actuarial Models",
    "section": "",
    "text": "In the ever evolving world of actuarial science, the traditional methods of building models and estimators don’t always stand up to the test of time. Advanced technology and understanding of models has led to the realization that bias is deeply embedded in populations. Luckily, we have new tools to identify and address this issue. In this guide, we will walk through what this bias may look like, what steps can be taken to account for it, and look at an example in code.\n\n1.0.0.1 What is Bias?\nBias is “a systematic distortion of a statistical result”, as described by Oxford Languages. In other words, bias is as aspect embedded in data that introduces an certain inclination or favor towards a result that otherwise would not exist without the bias.\nThere are many different kinds of bias, it can look different, be caused by different sources, and effects the data/model differently. Because of this variety, there is no single blanket test to run that can tell us if the model is biased or not. So to be a vigilant actuary, it is important to be aware of how bias may enter our model, how to look and check for bias, and how to address it.\nWhy Do We Care?\nBias can cause incorrect results in models if not addressed correctly. As modern day actuaries, we have a responsibility to build products that are fair and accurate. A poorly made model can cause a model’s algorithm to incorrectly deny claims, charge policyholders the wrong amount, or company to lose money. In a= delicate situation where small errors cause serious impacts on real people, actuaries are always expected to uphold a standard of quality work. In the words of Scott Priebe, the chief actuary at Pacific Life, “Actuaries are servants of the people first, and problem solvers second.”\nWhat Does Bias Look Like in Models?\nIt is important to know that bias can enter data during collection, selection, and design. Just because data is unbiased at one point does not mean it will be permanently. This is especially relevant if you are using data that someone else collected. So here is how we can spot some obvious bias, and later we’ll go over how to find the hidden bias.\n\nRead up on how the data was sampled. Some obvious problems may be obvious if you examine the population and sampling methods used to get the data.\n\nExample: Estimating the average income of adults in a city and only sampling from people living in houses. This would likely cause your estimate to be higher than it should be, because the data excludes anyone living in group housing, like apartments or retirement homes.\n\nVisualizing your data may show you an obvious problem with your data. Making a histogram or scatter plot can show you patterns, gaps, or discrepancies between groups. Depending on your data, it is a good idea to compare the response between any major factor (sex, race, economic status, etc.) as these are the most common places where bias can arise.\n\nIf you need help with visualization, here are some places to start:\nhttps://www.geeksforgeeks.org/data-visualization-in-r/\nhttps://r-graph-gallery.com/ggplot2-package.html\nhttps://libguides.princeton.edu/R-Visualization\n\n\n\n\n1.0.0.2 Data Example\nTo get a good idea on what getting started might look like, I will introduce some example data and the model I made from it.\nFirst, I’m going generate data with a clear gender bias. I’ll work with this simple example for the entire guide to show what each step looks like in code. The data I made includes age, gender, numbers of claims, and the premium charged. I then put together a model and created a column of claims that are predicted using the model.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nset.seed(123)\nn <- 500\n\nage <- sample(18:70, n, replace = TRUE)\ngender <- sample(c(\"Male\", \"Female\"), n, replace = TRUE)\nclaims <- rpois(n, lambda = 1.2)\npremium_base <- 200\nage_effect <- -3\nclaim_effect <- 50\n\npremiums <- premium_base + (age_effect * (age - 30)) + (claim_effect * claims) + rnorm(n, 0, 50)\n\ninsurance_data <- data.frame(age, gender, claims, premiums)\n\n#Premiums are charged at a higher rate for females\ninsurance_data$premiums <- ifelse(insurance_data$gender == \"Female\",\n                                 insurance_data$premiums * 1.2,\n                                 insurance_data$premiums)\n\nmodel = lm(premiums ~ age + factor(gender) + claims, data = insurance_data)\nsummary(model)\n\n\nCall:\nlm(formula = premiums ~ age + factor(gender) + claims, data = insurance_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-151.411  -39.441    0.475   33.017  190.876 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        352.5968     8.7808  40.155   <2e-16 ***\nage                 -3.5359     0.1668 -21.192   <2e-16 ***\nfactor(gender)Male -45.9716     4.9924  -9.208   <2e-16 ***\nclaims              56.5022     2.2818  24.762   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.71 on 496 degrees of freedom\nMultiple R-squared:  0.6952,    Adjusted R-squared:  0.6933 \nF-statistic: 377.1 on 3 and 496 DF,  p-value: < 2.2e-16\n\n# Predict premium\ninsurance_data$predicted_premiums <- predict(model, newdata = insurance_data)\n\n\n\n1.0.0.3 Stop and Think About Your Model\nBefore you jump into analysis, really think about how your model was built. Who collected the data? Who funded the collection? What was the motivation? Underlying bias can easily be sourced back to one of these reasons.\nWhere to look for Bias\nIn actuarial models, it is most important in products to assess if policyholders are being charged the correct amount. The goal in assessing bias is seeing if two groups who are very similar with only one difference are being charged differently. With my example data, we will look at if men and women with a similar profile are being charged differently. In reality, factors like accident history, location, vehicle type, and credit score are all considered in premium cost. Some of these factors are not independent and require a more complex model to be accurately fitted. If you do not have a model built from your data here are some resources: https://www.datacamp.com/tutorial/linear-regression-R\nIf you have dependent or categorical variables that you need to have in your model:\nhttps://www.statology.org/multicollinearity-in-r/\nhttps://www.sthda.com/english/articles/40-regression-analysis/163-regression-with-categorical-variables-dummy-coding-essentials-in-r/\nAssumptions\nLastly, there are some important assumption for your data before you start an analysis.\n\nData should be sufficiently large. While there is not exact threshold, there should at least be over 100 observations. Here is a resource for learning more about selecting sample size: https://pmc.ncbi.nlm.nih.gov/articles/PMC4148275/\nIf there are subgroups within your data, there should be equal representation of these groups. You can verify this by checking about the data were sampled (blocks, clusters, random sampling).\nMake sure your data (or model) are not missing any relevant variables that influence the response (like age for insurance pricing).\nBe careful of missing values, and look at where more values are missing before removing them. It’s common to have more missing values in low-income participants, and you could accidentally introduce bias by removing them. If you think you have sensitive missing values, here a two sources to help handle them: https://pmc.ncbi.nlm.nih.gov/articles/PMC3668100/, https://rpubs.com/chibueze99/MissingR\nNormality, some tests used in this analysis make this assumption, so it is important to check your data first. Running certain tests on non-normal data can cause misleading results. Here is how you can check any numerical variables using the Shapiro-Wilks Test:\n\n\nshapiro.test(insurance_data$age)\n\n\n    Shapiro-Wilk normality test\n\ndata:  insurance_data$age\nW = 0.95714, p-value = 7.053e-11\n\n\nA p-value result that is significant (less that 0.05) means your variable is normally distributed.\nIf you found that one or many variables are not normal, here is a source to handle that data: https://www.isixsigma.com/normality/dealing-non-normal-data-strategies-and-tools/\nNote that if the p-value is between 0.05 and 0.1, you may still say your data is approximately normal, but returning to the visualization step and looking at the distributions would be a good idea.\n\n\n1.0.0.4 Bias Analysis\nOur first steps into bias analysis are to apply fairness metrics where we think bias could be located. We can start by looking for direct disparity, meaning we are making comparisons between groups to see if we can find a difference that shouldn’t be there.\nVisual Comparisons\nI am first going to check what the premiums looks like between males and females.\n\nggplot(insurance_data, aes(x = premiums, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Premium Distribution by Gender\")\n\n\n\n\nLooking at this graph would raise a red flag to me. Based on previous research, we should expected that females get charged slightly less than males, but this graph tells us otherwise. Since I designed the data I know there is a gender bias, but with real data, this is a definite sign to investigate further.\nDisparate Impact This test takes the ratio of the mean predicted premiums by gender. If the results are significantly greater than or less than 1, we can suspect bias between the groups.\n\nmean(insurance_data$predicted_premiums[insurance_data$gender == \"Female\"]) / \n  mean(insurance_data$predicted_premiums[insurance_data$gender == \"Male\"])\n\n[1] 1.214633\n\n\nSince the result of 1.266 is greater than 1, it suggests bias.\nDirect Disparity\nIf you’re still not sure if the bias is present, another simple way to check for bias between groups is to compare means, and then run a t-test to check for a significant difference.\n\ninsurance_data %>%\n  group_by(gender) %>%\n  summarise(mean_premiums = mean(premiums))\n\n# A tibble: 2 × 2\n  gender mean_premiums\n  <chr>          <dbl>\n1 Female          260.\n2 Male            214.\n\nt.test(premiums ~ gender, data = insurance_data)\n\n\n    Welch Two Sample t-test\n\ndata:  premiums by gender\nt = 5.2335, df = 481.14, p-value = 2.488e-07\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 28.70285 63.21201\nsample estimates:\nmean in group Female   mean in group Male \n            260.0785             214.1211 \n\n\nThe results of the t test are significant. This result is interpreted as we have enough evidence to conclude there is a significant difference between the mean premiums of men and women.\n\n\n1.0.0.5 Addressing Bias\nNow that we know there is a bias, we want to adjust our model accordingly so that all policyholders are charged a fair amount.\nModel Correction & Quality\nImportant Note: Every time you make big changes to your model, you should go back and rerun the previous tests again. Log and compare every version of a model to assess the quality and trade offs.\nWith the goal of removing the bias from the model, we can reweigh the model and then create a new model. The new model will exclude gender to minimize the bias.\n\ninsurance_data$weights <- 1 / prop.table(table(insurance_data$gender))[insurance_data$gender]\n\n\nmodel_weighted <- lm(premiums ~ age + claims, \n                     data = insurance_data, \n                     weights = insurance_data$weights)\nsummary(model_weighted)\n\n\nCall:\nlm(formula = premiums ~ age + claims, data = insurance_data, \n    weights = insurance_data$weights)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-202.457  -57.106   -3.597   50.404  303.090 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 325.4959     8.9415   36.40   <2e-16 ***\nage          -3.4670     0.1803  -19.23   <2e-16 ***\nclaims       57.3833     2.4658   23.27   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 85.21 on 497 degrees of freedom\nMultiple R-squared:  0.6432,    Adjusted R-squared:  0.6418 \nF-statistic: 448.1 on 2 and 497 DF,  p-value: < 2.2e-16\n\n\nWhen looking at the summary of the new weighted model, we can see the adjusted R squared is smaller than it was before. The adjusted R squared is a metric of how well our model explains the data, so seeing is decrease it not ideal. What this means is that we create a more fair model, but we traded it for a less accurate model. This is a common problem for models with few predictors, and often removing a predictors is not the solution.\nNeed further help with your model? If the weighted model decrease the quality of your model, here are some other options for how to continue addressing the bias.\nPre-processing: https://medium.com/ibm-data-ai/fairness-in-machine-learning-pre-processing-algorithms-a670c031fba8\nPost-processing: https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/postprocessing.html\nMachine Learning: https://www.geeksforgeeks.org/bias-vs-variance-in-machine-learning/\nRandom Forest: https://kdagiit.medium.com/bagging-and-random-forests-reducing-bias-and-variance-using-randomness-8d516214fe7f\nUltimately what you decide to do with your model depends on what you value. What is your model going to be used for? Is fairness or accuracy more important? Do you trust your data to be accurate to the population? Here is a resource to learn more about making decisions for your model and what they mean in the actuarial world. https://www.actuary.org/sites/default/files/2023-07/risk_brief_data_bias.pdf\nFrom this article there is also a flow chart that may help with your process."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "::: {https://www.actuary.org/sites/default/files/2023-07/risk_brief_data_bias.pdf} ::: {https://www.geeksforgeeks.org/data-visualization-in-r/} ::: {https://r-graph-gallery.com/ggplot2-package.html} ::: {https://libguides.princeton.edu/R-Visualization} ::: {https://www.datacamp.com/tutorial/linear-regression-R ::: {https://www.statology.org/multicollinearity-in-r/} ::: {https://www.sthda.com/english/articles/40-regression-analysis/163-regression-with-categorical-variables-dummy-coding-essentials-in-r/} ::: {https://pmc.ncbi.nlm.nih.gov/articles/PMC4148275/} ::: {https://pmc.ncbi.nlm.nih.gov/articles/PMC3668100/, https://rpubs.com/chibueze99/MissingR} ::: {https://www.isixsigma.com/normality/dealing-non-normal-data-strategies-and-tools/} ::: https://medium.com/ibm-data-ai/fairness-in-machine-learning-pre-processing-algorithms-a670c031fba8 ::: {https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/postprocessing.html} ::: {https://www.geeksforgeeks.org/bias-vs-variance-in-machine-learning/} ::: {https://kdagiit.medium.com/bagging-and-random-forests-reducing-bias-and-variance-using-randomness-8d516214fe7f}"
  },
  {
    "objectID": "topic.html",
    "href": "topic.html",
    "title": "Appendix A — Topic",
    "section": "",
    "text": "Pick some subset of the information covered in your annual review article that you are interested in exploring in more detail.\nIdentify the scope of your user guide - what will you cover? Specific software packages? When to use or not to use a specific technique?\nI want to cover bias in actuarial models, in the context of how systematic pricing bias shows up in insurance products. The guide would go over how to identify and test bias in models, different methods of fix/manage the bias. I might try to tie in handling and cleaning the data models are built on and assessing data quality. All of this guide would be using R, and maybe Python companion if time allows it."
  },
  {
    "objectID": "needs.html",
    "href": "needs.html",
    "title": "Appendix B — Needs Assessment",
    "section": "",
    "text": "What does someone trying to accomplish your chosen task need help with?\nI assume that someone accomplishing my task already has a model, but I want to provide a section in my guide to make sure they understand what the model means and how to use it. They would also need help with assessing bias and the methods needed to address bias in the model.\nWhat parts are likely to be tricky?\nThere are some forms of bias that are very unique to each situation, because you have to consider the predictors in the model. I can walk them through steps of analyzing their model, but it does take some careful consideration from the reader about the implications of their model. There is no general test to see if an insurance or pricing model is biased, but we can check specific aspects in the data and see how it performs in the model. A huge part of knowing a model’s quality is to test it as much as possible.\nWhat resources are already available on this topic that may be helpful? Look for e.g. software vignettes, package documentation, papers about software packages, and so on.\nThere is a r package “fairness” and “glmnet” that will be very useful, as well as “ggplot” for visualization. I will probably also use “dplyr”, but I would lean on an assumption that someone working with models and large data has seen “ggplot” and “dplyr”. I think I would like to create a model from simulated data and purposefully introduce bias into it. Then I can walk through the steps of checking the model and provide an example. I also found a very good article I will reference for the user guide called The Actuarial View of Data Bias Andrews et al. (2023). This will be very helpful for me to explain different kinds of bias and the “big picture” of why we are about bias. I will definitely also defer readers to this article for a more in-depth understanding.\n\n\n\n\nAndrews, Dorothy, Shawna Ackerman, Dennis Kapylou, Alex Esche, Liaw Huang, and Reese Mularz. 2023. “An Actuarial View of Data Bias: Definitions, Impacts, and Considerations,” July. https://www.actuary.org/sites/default/files/2023-07/risk_brief_data_bias.pdf."
  },
  {
    "objectID": "guide.html#what-is-bias",
    "href": "guide.html#what-is-bias",
    "title": "1  User Guide To Bias in Actuarial Models",
    "section": "1.1 What is Bias?",
    "text": "1.1 What is Bias?\nBias is “a systematic distortion of a statistical result”, as described by Oxford Languages. In other words, bias is as aspect embedded in data that introduces an certain inclination or favor towards a result that otherwise would not exist without the bias.\nThere are many different kinds of bias, it can look different, be caused by different sources, and effects the data/model differently. Because of this variety, there is no single blanket test to run that can tell us if the model is biased or not. So to be a vigilant actuary, it is important to be aware of how bias may enter our model, how to look and check for bias, and how to address it.\n\n1.1.1 Why Do We Care?\nBias can cause incorrect results in models if not addressed correctly. As modern day actuaries, we have a responsibility to build products that are fair and accurate. A poorly made model can cause a model’s algorithm to incorrectly deny claims, charge policyholders the wrong amount, or company to lose money. In a= delicate situation where small errors cause serious impacts on real people, actuaries are always expected to uphold a standard of quality work. In the words of Scott Priebe, the chief actuary at Pacific Life, “Actuaries are servants of the people first, and problem solvers second.”\n\n\n1.1.2 What Does Bias Look Like in Models?\nIt is important to know that bias can enter data during collection, selection, and design. Just because data is unbiased at one point does not mean it will be permanently. This is especially relevant if you are using data that someone else collected. So here is how we can spot some obvious bias, and later we’ll go over how to find the hidden bias.\n\nRead up on how the data was sampled. Some obvious problems may be obvious if you examine the population and sampling methods used to get the data.\n\nExample: Estimating the average income of adults in a city and only sampling from people living in houses. This would likely cause your estimate to be higher than it should be, because the data excludes anyone living in group housing, like apartments or retirement homes.\n\nVisualizing your data may show you an obvious problem with your data. Making a histogram or scatter plot can show you patterns, gaps, or discrepancies between groups. Depending on your data, it is a good idea to compare the response between any major factor (sex, race, economic status, etc.) as these are the most common places where bias can arise.\n\nIf you need help with visualization, here are some places to start:\nhttps://www.geeksforgeeks.org/data-visualization-in-r/\nhttps://r-graph-gallery.com/ggplot2-package.html\nhttps://libguides.princeton.edu/R-Visualization"
  },
  {
    "objectID": "guide.html#data-example",
    "href": "guide.html#data-example",
    "title": "1  User Guide To Bias in Actuarial Models",
    "section": "1.2 Data Example",
    "text": "1.2 Data Example\nTo get a good idea on what getting started might look like, I will introduce some example data and the model I made from it.\nFirst, I’m going generate data with a clear gender bias. I’ll work with this simple example for the entire guide to show what each step looks like in code. The data I made includes age, gender, numbers of claims, and the premium charged. I then put together a model and created a column of claims that are predicted using the model.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\nset.seed(123)\nn <- 500\n\nage <- sample(18:70, n, replace = TRUE)\ngender <- sample(c(\"Male\", \"Female\"), n, replace = TRUE)\nclaims <- rpois(n, lambda = 1.2)\npremium_base <- 200\nage_effect <- -3\nclaim_effect <- 50\n\npremiums <- premium_base + (age_effect * (age - 30)) + (claim_effect * claims) + rnorm(n, 0, 50)\n\ninsurance_data <- data.frame(age, gender, claims, premiums)\n\n#Premiums are charged at a higher rate for females\ninsurance_data$premiums <- ifelse(insurance_data$gender == \"Female\",\n                                 insurance_data$premiums * 1.2,\n                                 insurance_data$premiums)\n\nmodel = lm(premiums ~ age + factor(gender) + claims, data = insurance_data)\nsummary(model)\n\n\nCall:\nlm(formula = premiums ~ age + factor(gender) + claims, data = insurance_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-151.411  -39.441    0.475   33.017  190.876 \n\nCoefficients:\n                   Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        352.5968     8.7808  40.155   <2e-16 ***\nage                 -3.5359     0.1668 -21.192   <2e-16 ***\nfactor(gender)Male -45.9716     4.9924  -9.208   <2e-16 ***\nclaims              56.5022     2.2818  24.762   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 55.71 on 496 degrees of freedom\nMultiple R-squared:  0.6952,    Adjusted R-squared:  0.6933 \nF-statistic: 377.1 on 3 and 496 DF,  p-value: < 2.2e-16\n\n# Predict premium\ninsurance_data$predicted_premiums <- predict(model, newdata = insurance_data)\n\n\n1.2.1 Stop and Think About Your Model\nBefore you jump into analysis, really think about how your model was built. Who collected the data? Who funded the collection? What was the motivation? Underlying bias can easily be sourced back to one of these reasons.\n\n\n1.2.2 Where to look for Bias\nIn actuarial models, it is most important in products to assess if policyholders are being charged the correct amount. The goal in assessing bias is seeing if two groups who are very similar with only one difference are being charged differently. With my example data, we will look at if men and women with a similar profile are being charged differently. In reality, factors like accident history, location, vehicle type, and credit score are all considered in premium cost. Some of these factors are not independent and require a more complex model to be accurately fitted. If you do not have a model built from your data here are some resources: https://www.datacamp.com/tutorial/linear-regression-R\nIf you have dependent or categorical variables that you need to have in your model:\nhttps://www.statology.org/multicollinearity-in-r/\nhttps://www.sthda.com/english/articles/40-regression-analysis/163-regression-with-categorical-variables-dummy-coding-essentials-in-r/\n\n\n1.2.3 Assumptions\nLastly, there are some important assumption for your data before you start an analysis.\n\nData should be sufficiently large. While there is not exact threshold, there should at least be over 100 observations. Here is a resource for learning more about selecting sample size: https://pmc.ncbi.nlm.nih.gov/articles/PMC4148275/\nIf there are subgroups within your data, there should be equal representation of these groups. You can verify this by checking about the data were sampled (blocks, clusters, random sampling).\nMake sure your data (or model) are not missing any relevant variables that influence the response (like age for insurance pricing).\nBe careful of missing values, and look at where more values are missing before removing them. It’s common to have more missing values in low-income participants, and you could accidentally introduce bias by removing them. If you think you have sensitive missing values, here a two sources to help handle them: https://pmc.ncbi.nlm.nih.gov/articles/PMC3668100/, https://rpubs.com/chibueze99/MissingR\nNormality, some tests used in this analysis make this assumption, so it is important to check your data first. Running certain tests on non-normal data can cause misleading results. Here is how you can check any numerical variables using the Shapiro-Wilks Test:\n\n\nshapiro.test(insurance_data$age)\n\n\n    Shapiro-Wilk normality test\n\ndata:  insurance_data$age\nW = 0.95714, p-value = 7.053e-11\n\n\nA p-value result that is significant (less that 0.05) means your variable is normally distributed.\nIf you found that one or many variables are not normal, here is a source to handle that data: https://www.isixsigma.com/normality/dealing-non-normal-data-strategies-and-tools/\nNote that if the p-value is between 0.05 and 0.1, you may still say your data is approximately normal, but returning to the visualization step and looking at the distributions would be a good idea."
  },
  {
    "objectID": "guide.html#bias-analysis",
    "href": "guide.html#bias-analysis",
    "title": "1  User Guide To Bias in Actuarial Models",
    "section": "1.3 Bias Analysis",
    "text": "1.3 Bias Analysis\nOur first steps into bias analysis are to apply fairness metrics where we think bias could be located. We can start by looking for direct disparity, meaning we are making comparisons between groups to see if we can find a difference that shouldn’t be there.\n\n1.3.1 Visual Comparisons\nI am first going to check what the premiums looks like between males and females.\n\nggplot(insurance_data, aes(x = premiums, fill = gender)) +\n  geom_density(alpha = 0.5) +\n  labs(title = \"Premium Distribution by Gender\")\n\n\n\n\nLooking at this graph would raise a red flag to me. Based on previous research, we should expected that females get charged slightly less than males, but this graph tells us otherwise. Since I designed the data I know there is a gender bias, but with real data, this is a definite sign to investigate further.\n\n\n1.3.2 Disparate Impact\nThis test takes the ratio of the mean predicted premiums by gender. If the results are significantly greater than or less than 1, we can suspect bias between the groups.\n\nmean(insurance_data$predicted_premiums[insurance_data$gender == \"Female\"]) / \n  mean(insurance_data$predicted_premiums[insurance_data$gender == \"Male\"])\n\n[1] 1.214633\n\n\nSince the result of 1.266 is greater than 1, it suggests bias.\n\n\n1.3.3 Direct Disparity\nIf you’re still not sure if the bias is present, another simple way to check for bias between groups is to compare means, and then run a t-test to check for a significant difference.\n\ninsurance_data %>%\n  group_by(gender) %>%\n  summarise(mean_premiums = mean(premiums))\n\n# A tibble: 2 × 2\n  gender mean_premiums\n  <chr>          <dbl>\n1 Female          260.\n2 Male            214.\n\nt.test(premiums ~ gender, data = insurance_data)\n\n\n    Welch Two Sample t-test\n\ndata:  premiums by gender\nt = 5.2335, df = 481.14, p-value = 2.488e-07\nalternative hypothesis: true difference in means between group Female and group Male is not equal to 0\n95 percent confidence interval:\n 28.70285 63.21201\nsample estimates:\nmean in group Female   mean in group Male \n            260.0785             214.1211 \n\n\nThe results of the t test are significant. This result is interpreted as we have enough evidence to conclude there is a significant difference between the mean premiums of men and women."
  },
  {
    "objectID": "guide.html#addressing-bias",
    "href": "guide.html#addressing-bias",
    "title": "1  User Guide To Bias in Actuarial Models",
    "section": "1.4 Addressing Bias",
    "text": "1.4 Addressing Bias\nNow that we know there is a bias, we want to adjust our model accordingly so that all policyholders are charged a fair amount.\n\n1.4.1 Model Correction & Quality\nImportant Note: Every time you make big changes to your model, you should go back and rerun the previous tests again. Log and compare every version of a model to assess the quality and trade offs.\nWith the goal of removing the bias from the model, we can reweigh the model and then create a new model. The new model will exclude gender to minimize the bias.\n\ninsurance_data$weights <- 1 / prop.table(table(insurance_data$gender))[insurance_data$gender]\n\n\nmodel_weighted <- lm(premiums ~ age + claims, \n                     data = insurance_data, \n                     weights = insurance_data$weights)\nsummary(model_weighted)\n\n\nCall:\nlm(formula = premiums ~ age + claims, data = insurance_data, \n    weights = insurance_data$weights)\n\nWeighted Residuals:\n     Min       1Q   Median       3Q      Max \n-202.457  -57.106   -3.597   50.404  303.090 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 325.4959     8.9415   36.40   <2e-16 ***\nage          -3.4670     0.1803  -19.23   <2e-16 ***\nclaims       57.3833     2.4658   23.27   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 85.21 on 497 degrees of freedom\nMultiple R-squared:  0.6432,    Adjusted R-squared:  0.6418 \nF-statistic: 448.1 on 2 and 497 DF,  p-value: < 2.2e-16\n\n\nWhen looking at the summary of the new weighted model, we can see the adjusted R squared is smaller than it was before. The adjusted R squared is a metric of how well our model explains the data, so seeing is decrease it not ideal. What this means is that we create a more fair model, but we traded it for a less accurate model. This is a common problem for models with few predictors, and often removing a predictors is not the solution.\n\n\n1.4.2 Need further help with your model?\nIf the weighted model decrease the quality of your model, here are some other options for how to continue addressing the bias.\nPre-processing: https://medium.com/ibm-data-ai/fairness-in-machine-learning-pre-processing-algorithms-a670c031fba8\nPost-processing: https://holisticai.readthedocs.io/en/latest/getting_started/bias/mitigation/postprocessing.html\nMachine Learning: https://www.geeksforgeeks.org/bias-vs-variance-in-machine-learning/\nRandom Forest: https://kdagiit.medium.com/bagging-and-random-forests-reducing-bias-and-variance-using-randomness-8d516214fe7f\nUltimately what you decide to do with your model depends on what you value. What is your model going to be used for? Is fairness or accuracy more important? Do you trust your data to be accurate to the population? Here is a resource to learn more about making decisions for your model and what they mean in the actuarial world. https://www.actuary.org/sites/default/files/2023-07/risk_brief_data_bias.pdf\nFrom this article there is also a flow chart that may help with your process."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "User Guide: Bias in Actuarial Models",
    "section": "",
    "text": "Assignment Flow\nThis project is set up to scaffold this user guide and set you up for success.\nStart at topic.qmd and identify a topic for your user guide.\nOnce you’ve proposed a topic and received my approval/feedback, you can convert the contents of topic.qmd into an introductory paragraph in your report. Please leave the topic.qmd file as an appendix, to document the different stages of this project.\nOnce we’ve agreed on a topic for your user guide, you will proceed to needs.qmd and task-analysis.qmd. Both will be submitted at the same check-in, and both will be included in your final report as appendices. You should be able to re-purpose most of the information in task-analysis.qmd to orient the user to different components of the task you’ve chosen.\nYour next step is to actually write the content that satisfies the work you outlined in task-analysis.qmd.\nOnce you’re finished with that, ideally, you’ll have plenty of time to edit and streamline your report. Feel free to trade reports with a friend and try to complete your friend’s task - this will help you both identify areas where the guide isn’t as clear. Try to complete the task with a different data set - that often helps find trouble spots.\nWhen you submit your final report, you may remove index.qmd from the _quarto.yml file, which will remove this chapter (which is quite unnecessary for your user) from the report. You should also take the time to make sure your name is listed as both author and copyright holder in the _quarto.yml file. Feel free to make a custom cover for your manual if you would like to do so. You can also tweak the CSS/theme for the book, so long as you’re conscious of accessibility concerns.\n\n\n\n\n\n\nBuilding the report\n\n\n\nIf you are using RStudio to complete this report, you can hit (Ctrl/Cmd)-Shift-B to build the whole report. You can also type quarto render . on the command line in the project folder to accomplish the same thing.\nIf you have questions about how to customize or debug your book, it may be helpful to start at the quarto book documentation (positpbcCreatingBook2024?)."
  }
]